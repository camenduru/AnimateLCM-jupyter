{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/AnimateLCM-jupyter/blob/main/AnimateLCM_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b old https://github.com/camenduru/AnimateLCM-hf\n",
        "%cd /content/AnimateLCM-hf\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/wangfuyun/AnimateLCM/resolve/main/models/DreamBooth_LoRA/cartoon2d.safetensors -d /content/AnimateLCM-hf/models/DreamBooth_LoRA -o cartoon2d.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/wangfuyun/AnimateLCM/resolve/main/models/DreamBooth_LoRA/cartoon3d.safetensors -d /content/AnimateLCM-hf/models/DreamBooth_LoRA -o cartoon3d.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/wangfuyun/AnimateLCM/resolve/main/models/DreamBooth_LoRA/realistic1.safetensors -d /content/AnimateLCM-hf/models/DreamBooth_LoRA -o realistic1.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/wangfuyun/AnimateLCM/resolve/main/models/DreamBooth_LoRA/realistic2.safetensors -d /content/AnimateLCM-hf/models/DreamBooth_LoRA -o realistic2.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/wangfuyun/AnimateLCM/resolve/main/models/LCM_LoRA/sd15_t2v_beta_lora.safetensors -d /content/AnimateLCM-hf/models/LCM_LoRA -o sd15_t2v_beta_lora.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/wangfuyun/AnimateLCM/resolve/main/models/Motion_Module/sd15_t2v_beta_motion.ckpt -d /content/AnimateLCM-hf/models/Motion_Module -o sd15_t2v_beta_motion.ckpt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/wangfuyun/AnimateLCM/resolve/main/models/StableDiffusion/stable-diffusion-v1-5/text_encoder/model.safetensors -d /content/AnimateLCM-hf/models/StableDiffusion/stable-diffusion-v1-5/text_encoder -o model.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/wangfuyun/AnimateLCM/resolve/main/models/StableDiffusion/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin -d /content/AnimateLCM-hf/models/StableDiffusion/stable-diffusion-v1-5/unet -o diffusion_pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/spaces/wangfuyun/AnimateLCM/resolve/main/models/StableDiffusion/stable-diffusion-v1-5/vae/diffusion_pytorch_model.bin -d /content/AnimateLCM-hf/models/StableDiffusion/stable-diffusion-v1-5/vae -o diffusion_pytorch_model.bin\n",
        "\n",
        "!pip install gradio==3.50.2 omegaconf diffusers==0.11.1 transformers==4.25.1 einops accelerate\n",
        "!pip install -q https://download.pytorch.org/whl/cu121/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "\n",
        "from glob import glob\n",
        "from omegaconf import OmegaConf\n",
        "from datetime import datetime\n",
        "from safetensors import safe_open\n",
        "\n",
        "from diffusers import AutoencoderKL\n",
        "from diffusers.utils.import_utils import is_xformers_available\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from animatelcm.scheduler.lcm_scheduler import LCMScheduler\n",
        "from animatelcm.models.unet import UNet3DConditionModel\n",
        "from animatelcm.pipelines.pipeline_animation import AnimationPipeline\n",
        "from animatelcm.utils.util import save_videos_grid\n",
        "from animatelcm.utils.convert_from_ckpt import convert_ldm_unet_checkpoint, convert_ldm_clip_checkpoint, convert_ldm_vae_checkpoint\n",
        "from animatelcm.utils.convert_lora_safetensor_to_diffusers import convert_lora\n",
        "from animatelcm.utils.lcm_utils import convert_lcm_lora\n",
        "import copy\n",
        "\n",
        "sample_idx = 0\n",
        "scheduler_dict = {\n",
        "    \"LCM\": LCMScheduler,\n",
        "}\n",
        "\n",
        "class AnimateController:\n",
        "    def __init__(self):\n",
        "\n",
        "        # config dirs\n",
        "        self.basedir = os.getcwd()\n",
        "        self.stable_diffusion_dir = os.path.join(\n",
        "            self.basedir, \"models\", \"StableDiffusion\")\n",
        "        self.motion_module_dir = os.path.join(\n",
        "            self.basedir, \"models\", \"Motion_Module\")\n",
        "        self.personalized_model_dir = os.path.join(\n",
        "            self.basedir, \"models\", \"DreamBooth_LoRA\")\n",
        "        self.savedir = os.path.join(\n",
        "            self.basedir, \"samples\", datetime.now().strftime(\"Gradio-%Y-%m-%dT%H-%M-%S\"))\n",
        "        self.savedir_sample = os.path.join(self.savedir, \"sample\")\n",
        "        self.lcm_lora_path = \"models/LCM_LoRA/sd15_t2v_beta_lora.safetensors\"\n",
        "        os.makedirs(self.savedir, exist_ok=True)\n",
        "\n",
        "        self.stable_diffusion_list = []\n",
        "        self.motion_module_list = []\n",
        "        self.personalized_model_list = []\n",
        "\n",
        "        self.refresh_stable_diffusion()\n",
        "        self.refresh_motion_module()\n",
        "        self.refresh_personalized_model()\n",
        "\n",
        "        # config models\n",
        "        self.tokenizer = None\n",
        "        self.text_encoder = None\n",
        "        self.vae = None\n",
        "        self.unet = None\n",
        "        self.pipeline = None\n",
        "        self.lora_model_state_dict = {}\n",
        "\n",
        "        self.inference_config = OmegaConf.load(\"configs/inference.yaml\")\n",
        "\n",
        "    def refresh_stable_diffusion(self):\n",
        "        self.stable_diffusion_list = glob(\n",
        "            os.path.join(self.stable_diffusion_dir, \"*/\"))\n",
        "\n",
        "    def refresh_motion_module(self):\n",
        "        motion_module_list = glob(os.path.join(\n",
        "            self.motion_module_dir, \"*.ckpt\"))\n",
        "        self.motion_module_list = [\n",
        "            os.path.basename(p) for p in motion_module_list]\n",
        "\n",
        "    def refresh_personalized_model(self):\n",
        "        personalized_model_list = glob(os.path.join(\n",
        "            self.personalized_model_dir, \"*.safetensors\"))\n",
        "        self.personalized_model_list = [\n",
        "            os.path.basename(p) for p in personalized_model_list]\n",
        "\n",
        "    def update_stable_diffusion(self, stable_diffusion_dropdown):\n",
        "        stable_diffusion_dropdown = os.path.join(self.stable_diffusion_dir,stable_diffusion_dropdown)\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(stable_diffusion_dropdown, subfolder=\"tokenizer\")\n",
        "        self.text_encoder = CLIPTextModel.from_pretrained(stable_diffusion_dropdown, subfolder=\"text_encoder\").cuda()\n",
        "        self.vae = AutoencoderKL.from_pretrained(stable_diffusion_dropdown, subfolder=\"vae\").cuda()\n",
        "        self.unet = UNet3DConditionModel.from_pretrained_2d(stable_diffusion_dropdown, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(self.inference_config.unet_additional_kwargs)).cuda()\n",
        "\n",
        "    def update_motion_module(self, motion_module_dropdown):\n",
        "        motion_module_dropdown = os.path.join(self.motion_module_dir, motion_module_dropdown)\n",
        "        motion_module_state_dict = torch.load(motion_module_dropdown, map_location=\"cpu\")\n",
        "        missing, unexpected = self.unet.load_state_dict(motion_module_state_dict, strict=False)\n",
        "\n",
        "    def update_base_model(self, base_model_dropdown):\n",
        "        base_model_dropdown = os.path.join(self.personalized_model_dir, base_model_dropdown)\n",
        "        base_model_state_dict = {}\n",
        "        with safe_open(base_model_dropdown, framework=\"pt\", device=\"cpu\") as f:\n",
        "            for key in f.keys():\n",
        "                base_model_state_dict[key] = f.get_tensor(key)\n",
        "        converted_vae_checkpoint = convert_ldm_vae_checkpoint(\n",
        "            base_model_state_dict, self.vae.config)\n",
        "        self.vae.load_state_dict(converted_vae_checkpoint)\n",
        "        converted_unet_checkpoint = convert_ldm_unet_checkpoint(base_model_state_dict, self.unet.config)\n",
        "        self.unet.load_state_dict(converted_unet_checkpoint, strict=False)\n",
        "\n",
        "    def update_lora_model(self, lora_model_dropdown):\n",
        "        lora_model_dropdown = os.path.join(\n",
        "            self.personalized_model_dir, lora_model_dropdown)\n",
        "        self.lora_model_state_dict = {}\n",
        "        if lora_model_dropdown == \"none\":\n",
        "            pass\n",
        "        else:\n",
        "            with safe_open(lora_model_dropdown, framework=\"pt\", device=\"cpu\") as f:\n",
        "                for key in f.keys():\n",
        "                    self.lora_model_state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "controller = AnimateController()\n",
        "\n",
        "controller.update_stable_diffusion(\"/content/AnimateLCM-hf/models/StableDiffusion/stable-diffusion-v1-5\")\n",
        "controller.update_motion_module(\"/content/AnimateLCM-hf/models/Motion_Module/sd15_t2v_beta_motion.ckpt\")\n",
        "controller.update_base_model(\"/content/AnimateLCM-hf/models/DreamBooth_LoRA/cartoon3d.safetensors\")\n",
        "# controller.update_lora_model(\"\")\n",
        "\n",
        "if is_xformers_available():\n",
        "    controller.unet.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "pipeline = AnimationPipeline(vae=controller.vae, text_encoder=controller.text_encoder, tokenizer=controller.tokenizer, unet=controller.unet,\n",
        "            scheduler=scheduler_dict[\"LCM\"](**OmegaConf.to_container(controller.inference_config.noise_scheduler_kwargs))).to(\"cuda\")\n",
        "\n",
        "if controller.lora_model_state_dict != {}:\n",
        "    pipeline = convert_lora(pipeline, controller.lora_model_state_dict, alpha=lora_alpha_slider)\n",
        "\n",
        "pipeline.unet = convert_lcm_lora(copy.deepcopy(controller.unet), controller.lcm_lora_path, 0.8)\n",
        "pipeline.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.seed()\n",
        "seed = torch.initial_seed()\n",
        "\n",
        "sample = pipeline(\n",
        "            'a cute rabbit, white background, pastel hues, minimal illustration, line art, pen drawing',\n",
        "            negative_prompt='negative_prompt_textbox',\n",
        "            num_inference_steps=4,\n",
        "            guidance_scale=1,\n",
        "            width=512,\n",
        "            height=512,\n",
        "            video_length=16,\n",
        "        ).videos\n",
        "\n",
        "save_sample_path = os.path.join(controller.savedir_sample, f\"{sample_idx}.mp4\")\n",
        "save_videos_grid(sample, save_sample_path)\n",
        "save_sample_path"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
